{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This assignment may be worked individually or in pairs. \n",
    "## Enter your name/names here:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Garrett Kelley glk447"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 2: Naive Bayes and KNN classifier\n",
    "\n",
    "In this assignment you'll implement the Naive Bayes and KNN classifiers to classify patients as either having or not having diabetic retinopathy. For this task we'll be using the same Diabetic Retinopathy data set which was used in the previous assignment on decision trees. The implementation details are up to you but, generally it is a good idea to divide your code up into helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Headers\n",
    "# You are welcome to add additional headers if you wish\n",
    "# EXCEPT for scikit-learn... You may NOT use scikit-learn for this assignment!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from a CSV file. You may choose to store it any any format you wish, like a Pandas dataframe, or any other data structure you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    data = []\n",
    "    data = pd.read_csv(filename, header=None)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Naive Bayes Classifier\n",
    "\n",
    "Naive Bayes (NB) classifier is a simple probabilistic classifier that is based on applying the Bayes' theorem and assumes a strong (naive) independence between features. The Diabetic Retinopathy data set contains both categorical and continuous features. Dealing with categorical features has been discussed in detail in class. Continuous attributes, on the other hand, are more interesting to handle. Most commonly, this is done by assuming normal probability distribution over the feature values or by binning the attribute values in a fixed number of bins. In this assignment you'll be implementing the binning approach. For each continuous attribute, you'll construct 3 equal sized bins. For example, feature 5 ranges from `[1 - 120]` the 3 bins that you'll construct will be `[1 - 40]`, `[41 - 80]`, `[81 - 120]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Implement a Naive Bayes classifier. Measure the accuracy of your classifier using 5-fold cross validation and display the confusion matrix. Also print the precision and recall for class label 1 (patients that have been diagnosed with the disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy fold  0 :  0.6695652173913044\n",
      "accuracy fold  1 :  0.6200873362445415\n",
      "accuracy fold  2 :  0.5720524017467249\n",
      "accuracy fold  3 :  0.5807860262008734\n",
      "accuracy fold  4 :  0.6069868995633187\n",
      "average accuracy:  0.6098955762293526\n",
      "confusion matrix:\n",
      " [[318. 230.]\n",
      " [222. 381.]]\n",
      "precision class 1:  0.6318407960199005\n",
      "recall class 1:  0.6235679214402619\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "data = get_data('messidor_features.txt')\n",
    "\n",
    "# initialize probability matrices for each feature using laplace smoothing\n",
    "feature_0_matrix = np.zeros((3, 2))\n",
    "feature_0_matrix[0:2][:] = 1\n",
    "feature_0_matrix[2][:] = 2\n",
    "feature_1_matrix = np.zeros((3, 2))\n",
    "feature_1_matrix[0:2][:] = 1\n",
    "feature_1_matrix[2][:] = 2\n",
    "feature_2_matrix = np.zeros((4, 2))\n",
    "feature_2_matrix[0:3][:] = 1\n",
    "feature_2_matrix[3][:] = 3\n",
    "feature_2_min = data.iloc[:, 2].min()\n",
    "feature_2_max = data.iloc[:, 2].max()\n",
    "feature_3_matrix = np.zeros((4, 2))\n",
    "feature_3_matrix[0:3][:] = 1\n",
    "feature_3_matrix[3][:] = 3\n",
    "feature_3_min = data.iloc[:, 3].min()\n",
    "feature_3_max = data.iloc[:, 3].max()\n",
    "feature_4_matrix = np.zeros((4, 2))\n",
    "feature_4_matrix[0:3][:] = 1\n",
    "feature_4_matrix[3][:] = 3\n",
    "feature_4_min = data.iloc[:, 4].min()\n",
    "feature_4_max = data.iloc[:, 4].max()\n",
    "feature_5_matrix = np.zeros((4, 2))\n",
    "feature_5_matrix[0:3][:] = 1\n",
    "feature_5_matrix[3][:] = 3\n",
    "feature_5_min = data.iloc[:, 5].min()\n",
    "feature_5_max = data.iloc[:, 5].max()\n",
    "\n",
    "feature_6_matrix = np.zeros((4, 2))\n",
    "feature_6_matrix[0:3][:] = 1\n",
    "feature_6_matrix[3][:] = 3\n",
    "feature_6_min = data.iloc[:, 6].min()\n",
    "feature_6_max = data.iloc[:, 6].max()\n",
    "\n",
    "feature_7_matrix = np.zeros((4, 2))\n",
    "feature_7_matrix[0:3][:] = 1\n",
    "feature_7_matrix[3][:] = 3\n",
    "feature_7_min = data.iloc[:, 7].min()\n",
    "feature_7_max = data.iloc[:, 7].max()\n",
    "\n",
    "feature_8_matrix = np.zeros((4, 2))\n",
    "feature_8_matrix[0:3][:] = 1\n",
    "feature_8_matrix[3][:] = 3\n",
    "feature_8_min = data.iloc[:, 8].min()\n",
    "feature_8_max = data.iloc[:, 8].max()\n",
    "\n",
    "feature_9_matrix = np.zeros((4, 2))\n",
    "feature_9_matrix[0:3][:] = 1\n",
    "feature_9_matrix[3][:] = 3\n",
    "feature_9_min = data.iloc[:, 9].min()\n",
    "feature_9_max = data.iloc[:, 9].max()\n",
    "\n",
    "feature_10_matrix = np.zeros((4, 2))\n",
    "feature_10_matrix[0:3][:] = 1\n",
    "feature_10_matrix[3][:] = 3\n",
    "feature_10_min = data.iloc[:, 10].min()\n",
    "feature_10_max = data.iloc[:, 10].max()\n",
    "\n",
    "feature_11_matrix = np.zeros((4, 2))\n",
    "feature_11_matrix[0:3][:] = 1\n",
    "feature_11_matrix[3][:] = 3\n",
    "feature_11_min = data.iloc[:, 11].min()\n",
    "feature_11_max = data.iloc[:, 11].max()\n",
    "\n",
    "feature_12_matrix = np.zeros((4, 2))\n",
    "feature_12_matrix[0:3][:] = 1\n",
    "feature_12_matrix[3][:] = 3\n",
    "feature_12_min = data.iloc[:, 12].min()\n",
    "feature_12_max = data.iloc[:, 12].max()\n",
    "\n",
    "feature_13_matrix = np.zeros((4, 2))\n",
    "feature_13_matrix[0:3][:] = 1\n",
    "feature_13_matrix[3][:] = 3\n",
    "feature_13_min = data.iloc[:, 13].min()\n",
    "feature_13_max = data.iloc[:, 13].max()\n",
    "\n",
    "feature_14_matrix = np.zeros((4, 2))\n",
    "feature_14_matrix[0:3][:] = 1\n",
    "feature_14_matrix[3][:] = 3\n",
    "feature_14_min = data.iloc[:, 14].min()\n",
    "feature_14_max = data.iloc[:, 14].max()\n",
    "\n",
    "feature_15_matrix = np.zeros((4, 2))\n",
    "feature_15_matrix[0:3][:] = 1\n",
    "feature_15_matrix[3][:] = 3\n",
    "feature_15_min = data.iloc[:, 15].min()\n",
    "feature_15_max = data.iloc[:, 15].max()\n",
    "\n",
    "feature_16_matrix = np.zeros((4, 2))\n",
    "feature_16_matrix[0:3][:] = 1\n",
    "feature_16_matrix[3][:] = 3\n",
    "feature_16_min = data.iloc[:, 16].min()\n",
    "feature_16_max = data.iloc[:, 16].max()\n",
    "\n",
    "feature_17_matrix = np.zeros((4, 2))\n",
    "feature_17_matrix[0:3][:] = 1\n",
    "feature_17_matrix[3][:] = 3\n",
    "feature_17_min = data.iloc[:, 17].min()\n",
    "feature_17_max = data.iloc[:, 17].max()\n",
    "\n",
    "# initialize probability matrices for each feature using data\n",
    "for index, data_point in data.iterrows():\n",
    "    data_point_class = int(data_point[19])\n",
    "\n",
    "    # feature 0 probability\n",
    "    if data_point[0] == 0:\n",
    "        feature_0_matrix[0, data_point_class] = feature_0_matrix[0, data_point_class] + 1\n",
    "    else:\n",
    "        feature_0_matrix[1, data_point_class] = feature_0_matrix[1, data_point_class] + 1\n",
    "    feature_0_matrix[2, data_point_class] = feature_0_matrix[2, data_point_class] + 1\n",
    "    \n",
    "    # feature 1 probability\n",
    "    if data_point[1] == 0:\n",
    "        feature_1_matrix[0, data_point_class] = feature_1_matrix[0, data_point_class] + 1\n",
    "    else:\n",
    "        feature_1_matrix[1, data_point_class] = feature_1_matrix[1, data_point_class] + 1\n",
    "    feature_1_matrix[2, data_point_class] = feature_1_matrix[2, data_point_class] + 1\n",
    "\n",
    "    # feature 2 probability\n",
    "    if feature_2_min <= data_point[2] and data_point[2] <= feature_2_min + (feature_2_max - feature_2_min)/3.0:\n",
    "        feature_2_matrix[0, data_point_class] = feature_2_matrix[0, data_point_class] + 1\n",
    "    elif feature_2_min + (feature_2_max - feature_2_min)/3.0 <= data_point[2] and data_point[2] <= feature_2_min + 2.0*(feature_2_max - feature_2_min)/3.0:\n",
    "        feature_2_matrix[1, data_point_class] = feature_2_matrix[1, data_point_class] + 1\n",
    "    else:\n",
    "        feature_2_matrix[2, data_point_class] = feature_2_matrix[2, data_point_class] + 1\n",
    "    feature_2_matrix[3, data_point_class] = feature_2_matrix[3, data_point_class] + 1\n",
    "\n",
    "    # feature 3 probability\n",
    "    if feature_3_min <= data_point[3] and data_point[3] <= feature_3_min + (feature_3_max - feature_3_min)/3.0:\n",
    "        feature_3_matrix[0, data_point_class] = feature_3_matrix[0, data_point_class] + 1\n",
    "    elif feature_3_min + (feature_3_max - feature_3_min)/3.0 <= data_point[3] and data_point[3] <= feature_3_min + 2.0*(feature_3_max - feature_3_min)/3.0:\n",
    "        feature_3_matrix[1, data_point_class] = feature_3_matrix[1, data_point_class] + 1\n",
    "    else:\n",
    "        feature_3_matrix[2, data_point_class] = feature_3_matrix[2, data_point_class] + 1\n",
    "    feature_3_matrix[3, data_point_class] = feature_3_matrix[3, data_point_class] + 1 \n",
    "\n",
    "    # feature 4 probability\n",
    "    if feature_4_min <= data_point[4] and data_point[4] <= feature_4_min + (feature_4_max - feature_4_min)/3.0:\n",
    "        feature_4_matrix[0, data_point_class] = feature_4_matrix[0, data_point_class] + 1\n",
    "    elif feature_4_min + (feature_4_max - feature_4_min)/3.0 <= data_point[4] and data_point[4] <= feature_4_min + 2.0*(feature_4_max - feature_4_min)/3.0:\n",
    "        feature_4_matrix[1, data_point_class] = feature_4_matrix[1, data_point_class] + 1\n",
    "    else:\n",
    "        feature_4_matrix[2, data_point_class] = feature_4_matrix[2, data_point_class] + 1\n",
    "    feature_4_matrix[3, data_point_class] = feature_4_matrix[3, data_point_class] + 1 \n",
    "    \n",
    "    # feature 5 probability\n",
    "    if feature_5_min <= data_point[5] and data_point[5] <= feature_5_min + (feature_5_max - feature_5_min)/3.0:\n",
    "        feature_5_matrix[0, data_point_class] = feature_5_matrix[0, data_point_class] + 1\n",
    "    elif feature_5_min + (feature_5_max - feature_5_min)/3.0 <= data_point[5] and data_point[5] <= feature_5_min + 2.0*(feature_5_max - feature_5_min)/3.0:\n",
    "        feature_5_matrix[1, data_point_class] = feature_5_matrix[1, data_point_class] + 1\n",
    "    else:\n",
    "        feature_5_matrix[2, data_point_class] = feature_5_matrix[2, data_point_class] + 1\n",
    "    feature_5_matrix[3, data_point_class] = feature_5_matrix[3, data_point_class] + 1\n",
    "    \n",
    "    # feature 6 probability\n",
    "    if feature_6_min <= data_point[6] and data_point[6] <= feature_6_min + (feature_6_max - feature_6_min)/3.0:\n",
    "        feature_6_matrix[0, data_point_class] = feature_6_matrix[0, data_point_class] + 1\n",
    "    elif feature_6_min + (feature_6_max - feature_6_min)/3.0 <= data_point[6] and data_point[6] <= feature_6_min + 2.0*(feature_6_max - feature_6_min)/3.0:\n",
    "        feature_6_matrix[1, data_point_class] = feature_6_matrix[1, data_point_class] + 1\n",
    "    else:\n",
    "        feature_6_matrix[2, data_point_class] = feature_6_matrix[2, data_point_class] + 1\n",
    "    feature_6_matrix[3, data_point_class] = feature_6_matrix[3, data_point_class] + 1 \n",
    "        \n",
    "    # feature 7 probability\n",
    "    if feature_7_min <= data_point[7] and data_point[7] <= feature_7_min + (feature_7_max - feature_7_min)/3.0:\n",
    "        feature_7_matrix[0, data_point_class] = feature_7_matrix[0, data_point_class] + 1\n",
    "    elif feature_7_min + (feature_7_max - feature_7_min)/3.0 <= data_point[7] and data_point[7] <= feature_7_min + 2.0*(feature_7_max - feature_7_min)/3.0:\n",
    "        feature_7_matrix[1, data_point_class] = feature_7_matrix[1, data_point_class] + 1\n",
    "    else:\n",
    "        feature_7_matrix[2, data_point_class] = feature_7_matrix[2, data_point_class] + 1\n",
    "    feature_7_matrix[3, data_point_class] = feature_7_matrix[3, data_point_class] + 1\n",
    "    \n",
    "    # feature 8 probability\n",
    "    if feature_8_min <= data_point[8] and data_point[8] <= feature_8_min + (feature_8_max - feature_8_min)/3.0:\n",
    "        feature_8_matrix[0, data_point_class] = feature_8_matrix[0, data_point_class] + 1\n",
    "    elif feature_8_min + (feature_8_max - feature_8_min)/3.0 <= data_point[8] and data_point[8] <= feature_8_min + 2.0*(feature_8_max - feature_8_min)/3.0:\n",
    "        feature_8_matrix[1, data_point_class] = feature_8_matrix[1, data_point_class] + 1\n",
    "    else:\n",
    "        feature_8_matrix[2, data_point_class] = feature_8_matrix[2, data_point_class] + 1\n",
    "    feature_8_matrix[3, data_point_class] = feature_8_matrix[3, data_point_class] + 1     \n",
    "    \n",
    "    # feature 9 probability\n",
    "    if feature_9_min <= data_point[9] and data_point[9] <= feature_9_min + (feature_9_max - feature_9_min)/3.0:\n",
    "        feature_9_matrix[0, data_point_class] = feature_9_matrix[0, data_point_class] + 1\n",
    "    elif feature_9_min + (feature_9_max - feature_9_min)/3.0 <= data_point[9] and data_point[9] <= feature_9_min + 2.0*(feature_9_max - feature_9_min)/3.0:\n",
    "        feature_9_matrix[1, data_point_class] = feature_9_matrix[1, data_point_class] + 1\n",
    "    else:\n",
    "        feature_9_matrix[2, data_point_class] = feature_9_matrix[2, data_point_class] + 1\n",
    "    feature_9_matrix[3, data_point_class] = feature_9_matrix[3, data_point_class] + 1     \n",
    "    \n",
    "    # feature 10 probability\n",
    "    if feature_10_min <= data_point[10] and data_point[10] <= feature_10_min + (feature_10_max - feature_10_min)/3.0:\n",
    "        feature_10_matrix[0, data_point_class] = feature_10_matrix[0, data_point_class] + 1\n",
    "    elif feature_10_min + (feature_10_max - feature_10_min)/3.0 <= data_point[10] and data_point[10] <= feature_10_min + 2.0*(feature_10_max - feature_10_min)/3.0:\n",
    "        feature_10_matrix[1, data_point_class] = feature_10_matrix[1, data_point_class] + 1\n",
    "    else:\n",
    "        feature_10_matrix[2, data_point_class] = feature_10_matrix[2, data_point_class] + 1\n",
    "    feature_10_matrix[3, data_point_class] = feature_10_matrix[3, data_point_class] + 1     \n",
    "    \n",
    "    # feature 11 probability\n",
    "    if feature_11_min <= data_point[11] and data_point[11] <= feature_11_min + (feature_11_max - feature_11_min)/3.0:\n",
    "        feature_11_matrix[0, data_point_class] = feature_11_matrix[0, data_point_class] + 1\n",
    "    elif feature_11_min + (feature_11_max - feature_11_min)/3.0 <= data_point[11] and data_point[11] <= feature_11_min + 2.0*(feature_11_max - feature_11_min)/3.0:\n",
    "        feature_11_matrix[1, data_point_class] = feature_11_matrix[1, data_point_class] + 1\n",
    "    else:\n",
    "        feature_11_matrix[2, data_point_class] = feature_11_matrix[2, data_point_class] + 1\n",
    "    feature_11_matrix[3, data_point_class] = feature_11_matrix[3, data_point_class] + 1     \n",
    "    \n",
    "    # feature 12 probability\n",
    "    if feature_12_min <= data_point[12] and data_point[12] <= feature_12_min + (feature_12_max - feature_12_min)/3.0:\n",
    "        feature_12_matrix[0, data_point_class] = feature_12_matrix[0, data_point_class] + 1\n",
    "    elif feature_12_min + (feature_12_max - feature_12_min)/3.0 <= data_point[12] and data_point[12] <= feature_12_min + 2.0*(feature_12_max - feature_12_min)/3.0:\n",
    "        feature_12_matrix[1, data_point_class] = feature_12_matrix[1, data_point_class] + 1\n",
    "    else:\n",
    "        feature_12_matrix[2, data_point_class] = feature_12_matrix[2, data_point_class] + 1\n",
    "    feature_12_matrix[3, data_point_class] = feature_12_matrix[3, data_point_class] + 1     \n",
    "    \n",
    "    # feature 13 probability\n",
    "    if feature_13_min <= data_point[13] and data_point[13] <= feature_13_min + (feature_13_max - feature_13_min)/3.0:\n",
    "        feature_13_matrix[0, data_point_class] = feature_13_matrix[0, data_point_class] + 1\n",
    "    elif feature_13_min + (feature_13_max - feature_13_min)/3.0 <= data_point[13] and data_point[13] <= feature_13_min + 2.0*(feature_13_max - feature_13_min)/3.0:\n",
    "        feature_13_matrix[1, data_point_class] = feature_13_matrix[1, data_point_class] + 1\n",
    "    else:\n",
    "        feature_13_matrix[2, data_point_class] = feature_13_matrix[2, data_point_class] + 1\n",
    "    feature_13_matrix[3, data_point_class] = feature_13_matrix[3, data_point_class] + 1     \n",
    "    \n",
    "    # feature 14 probability\n",
    "    if feature_14_min <= data_point[14] and data_point[14] <= feature_14_min + (feature_14_max - feature_14_min)/3.0:\n",
    "        feature_14_matrix[0, data_point_class] = feature_14_matrix[0, data_point_class] + 1\n",
    "    elif feature_14_min + (feature_14_max - feature_14_min)/3.0 <= data_point[14] and data_point[14] <= feature_14_min + 2.0*(feature_14_max - feature_14_min)/3.0:\n",
    "        feature_14_matrix[1, data_point_class] = feature_14_matrix[1, data_point_class] + 1\n",
    "    else:\n",
    "        feature_14_matrix[2, data_point_class] = feature_14_matrix[2, data_point_class] + 1\n",
    "    feature_14_matrix[3, data_point_class] = feature_14_matrix[3, data_point_class] + 1    \n",
    "    \n",
    "    # feature 15 probability\n",
    "    if feature_15_min <= data_point[15] and data_point[15] <= feature_15_min + (feature_15_max - feature_15_min)/3.0:\n",
    "        feature_15_matrix[0, data_point_class] = feature_15_matrix[0, data_point_class] + 1\n",
    "    elif feature_15_min + (feature_15_max - feature_15_min)/3.0 <= data_point[15] and data_point[15] <= feature_15_min + 2.0*(feature_15_max - feature_15_min)/3.0:\n",
    "        feature_15_matrix[1, data_point_class] = feature_15_matrix[1, data_point_class] + 1\n",
    "    else:\n",
    "        feature_15_matrix[2, data_point_class] = feature_15_matrix[2, data_point_class] + 1\n",
    "    feature_15_matrix[3, data_point_class] = feature_15_matrix[3, data_point_class] + 1 \n",
    "    \n",
    "    # feature 16 probability\n",
    "    if feature_16_min <= data_point[16] and data_point[16] <= feature_16_min + (feature_16_max - feature_16_min)/3.0:\n",
    "        feature_16_matrix[0, data_point_class] = feature_16_matrix[0, data_point_class] + 1\n",
    "    elif feature_16_min + (feature_16_max - feature_16_min)/3.0 <= data_point[16] and data_point[16] <= feature_16_min + 2.0*(feature_16_max - feature_16_min)/3.0:\n",
    "        feature_16_matrix[1, data_point_class] = feature_16_matrix[1, data_point_class] + 1\n",
    "    else:\n",
    "        feature_16_matrix[2, data_point_class] = feature_16_matrix[2, data_point_class] + 1\n",
    "    feature_16_matrix[3, data_point_class] = feature_16_matrix[3, data_point_class] + 1     \n",
    "    \n",
    "    # feature 17 probability\n",
    "    if feature_17_min <= data_point[17] and data_point[17] <= feature_17_min + (feature_17_max - feature_17_min)/3.0:\n",
    "        feature_17_matrix[0, data_point_class] = feature_17_matrix[0, data_point_class] + 1\n",
    "    elif feature_17_min + (feature_17_max - feature_17_min)/3.0 <= data_point[17] and data_point[17] <= feature_17_min + 2.0*(feature_17_max - feature_17_min)/3.0:\n",
    "        feature_17_matrix[1, data_point_class] = feature_17_matrix[1, data_point_class] + 1\n",
    "    else:\n",
    "        feature_17_matrix[2, data_point_class] = feature_17_matrix[2, data_point_class] + 1\n",
    "    feature_17_matrix[3, data_point_class] = feature_17_matrix[3, data_point_class] + 1     \n",
    "    \n",
    "    \n",
    "\n",
    "# initialize probability of classes\n",
    "p_0 = data[19].value_counts()[0]/1151.0\n",
    "p_1 = data[19].value_counts()[1]/1151.0\n",
    "\n",
    "# initialize confusion matrix\n",
    "confusion_matrix = np.zeros((2, 2))\n",
    "\n",
    "# initialize accuracy matrix\n",
    "accuracy = np.zeros(5)\n",
    "\n",
    "# 5 fold cross validation loop\n",
    "start = 0\n",
    "end = int(1151/5)\n",
    "for i in range(0,5):\n",
    "    # split off test data\n",
    "    test_data = data.iloc[start:end + 1,:]\n",
    "    \n",
    "    # initialize correct predictions\n",
    "    num_correct = 0    \n",
    "    \n",
    "    # score each test instance\n",
    "    for index, data_point in test_data.iterrows():\n",
    "        # initialize class scores\n",
    "        class_0_score = p_0\n",
    "        class_1_score = p_1\n",
    "        \n",
    "        # calculate feature 0 probability\n",
    "        if data_point[0] == 0:\n",
    "            class_0_score = class_0_score*feature_0_matrix[0, 0]/feature_0_matrix[2, 0]\n",
    "            class_1_score = class_1_score*feature_0_matrix[0, 1]/feature_0_matrix[2, 1]\n",
    "        else:\n",
    "            class_0_score = class_0_score*feature_0_matrix[1, 0]/feature_0_matrix[2, 0]\n",
    "            class_1_score = class_1_score*feature_0_matrix[1, 1]/feature_0_matrix[2, 1]\n",
    "            \n",
    "        # feature 1 probability\n",
    "        if data_point[1] == 0:\n",
    "            class_0_score = class_0_score*feature_1_matrix[0, 0]/feature_1_matrix[2, 0]\n",
    "            class_1_score = class_1_score*feature_1_matrix[0, 1]/feature_1_matrix[2, 1]\n",
    "        else:\n",
    "            class_0_score = class_0_score*feature_1_matrix[0, 0]/feature_1_matrix[2, 0]\n",
    "            class_1_score = class_1_score*feature_1_matrix[0, 1]/feature_1_matrix[2, 1]\n",
    "\n",
    "        # feature 2 probability\n",
    "        if feature_2_min <= data_point[2] and data_point[2] <= feature_2_min + (feature_2_max - feature_2_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_2_matrix[0, 0]/feature_2_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_2_matrix[0, 1]/feature_2_matrix[3, 1]\n",
    "        elif feature_2_min + (feature_2_max - feature_2_min)/3.0 <= data_point[2] and data_point[2] <= feature_2_min + 2.0*(feature_2_max - feature_2_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_2_matrix[1, 0]/feature_2_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_2_matrix[1, 1]/feature_2_matrix[3, 1]\n",
    "        else:\n",
    "            class_0_score = class_0_score*feature_2_matrix[2, 0]/feature_2_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_2_matrix[2, 1]/feature_2_matrix[3, 1]\n",
    "            \n",
    "        # feature 3 probability\n",
    "        if feature_3_min <= data_point[3] and data_point[3] <= feature_3_min + (feature_3_max - feature_3_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_3_matrix[0, 0]/feature_3_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_3_matrix[0, 1]/feature_3_matrix[3, 1]\n",
    "        elif feature_3_min + (feature_3_max - feature_3_min)/3.0 <= data_point[3] and data_point[3] <= feature_3_min + 2.0*(feature_3_max - feature_3_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_3_matrix[1, 0]/feature_3_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_3_matrix[1, 1]/feature_3_matrix[3, 1]\n",
    "        else:\n",
    "            class_0_score = class_0_score*feature_3_matrix[2, 0]/feature_3_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_3_matrix[2, 1]/feature_3_matrix[3, 1] \n",
    "\n",
    "        # feature 4 probability\n",
    "        if feature_4_min <= data_point[4] and data_point[4] <= feature_4_min + (feature_4_max - feature_4_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_4_matrix[0, 0]/feature_4_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_4_matrix[0, 1]/feature_4_matrix[3, 1]        \n",
    "        elif feature_4_min + (feature_4_max - feature_4_min)/3.0 <= data_point[4] and data_point[4] <= feature_4_min + 2.0*(feature_4_max - feature_4_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_4_matrix[1, 0]/feature_4_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_4_matrix[1, 1]/feature_4_matrix[3, 1]\n",
    "        else:\n",
    "            class_0_score = class_0_score*feature_4_matrix[2, 0]/feature_4_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_4_matrix[2, 1]/feature_4_matrix[3, 1] \n",
    "        \n",
    "        # feature 5 probability\n",
    "        if feature_5_min <= data_point[5] and data_point[5] <= feature_5_min + (feature_5_max - feature_5_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_5_matrix[0, 0]/feature_5_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_5_matrix[0, 1]/feature_5_matrix[3, 1] \n",
    "        elif feature_5_min + (feature_5_max - feature_5_min)/3.0 <= data_point[5] and data_point[5] <= feature_5_min + 2.0*(feature_5_max - feature_5_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_5_matrix[1, 0]/feature_5_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_5_matrix[1, 1]/feature_5_matrix[3, 1]\n",
    "        else:\n",
    "            class_0_score = class_0_score*feature_5_matrix[2, 0]/feature_5_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_5_matrix[2, 1]/feature_5_matrix[3, 1]\n",
    "            \n",
    "        # feature 6 probability\n",
    "        if feature_6_min <= data_point[6] and data_point[6] <= feature_6_min + (feature_6_max - feature_6_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_6_matrix[0, 0]/feature_6_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_6_matrix[0, 1]/feature_6_matrix[3, 1]\n",
    "        elif feature_6_min + (feature_6_max - feature_6_min)/3.0 <= data_point[6] and data_point[6] <= feature_6_min + 2.0*(feature_6_max - feature_6_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_6_matrix[1, 0]/feature_6_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_6_matrix[1, 1]/feature_6_matrix[3, 1]        \n",
    "        else:\n",
    "            class_0_score = class_0_score*feature_6_matrix[2, 0]/feature_6_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_6_matrix[2, 1]/feature_6_matrix[3, 1]\n",
    "            \n",
    "        # feature 7 probability\n",
    "        if feature_7_min <= data_point[7] and data_point[7] <= feature_7_min + (feature_7_max - feature_7_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_7_matrix[0, 0]/feature_7_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_7_matrix[0, 1]/feature_7_matrix[3, 1]        \n",
    "        elif feature_7_min + (feature_7_max - feature_7_min)/3.0 <= data_point[7] and data_point[7] <= feature_7_min + 2.0*(feature_7_max - feature_7_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_7_matrix[1, 0]/feature_7_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_7_matrix[1, 1]/feature_7_matrix[3, 1] \n",
    "        else:\n",
    "            class_0_score = class_0_score*feature_7_matrix[2, 0]/feature_7_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_7_matrix[2, 1]/feature_7_matrix[3, 1]\n",
    "            \n",
    "        # feature 8 probability\n",
    "        if feature_8_min <= data_point[8] and data_point[8] <= feature_8_min + (feature_8_max - feature_8_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_8_matrix[0, 0]/feature_8_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_8_matrix[0, 1]/feature_8_matrix[3, 1]        \n",
    "        elif feature_8_min + (feature_8_max - feature_8_min)/3.0 <= data_point[8] and data_point[8] <= feature_8_min + 2.0*(feature_8_max - feature_8_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_8_matrix[1, 0]/feature_8_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_8_matrix[1, 1]/feature_8_matrix[3, 1]\n",
    "        else:\n",
    "            class_0_score = class_0_score*feature_8_matrix[2, 0]/feature_8_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_8_matrix[2, 1]/feature_8_matrix[3, 1]\n",
    "            \n",
    "        # feature 9 probability\n",
    "        if feature_9_min <= data_point[9] and data_point[9] <= feature_9_min + (feature_9_max - feature_9_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_9_matrix[0, 0]/feature_9_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_9_matrix[0, 1]/feature_9_matrix[3, 1]        \n",
    "        elif feature_9_min + (feature_9_max - feature_9_min)/3.0 <= data_point[9] and data_point[9] <= feature_9_min + 2.0*(feature_9_max - feature_9_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_9_matrix[1, 0]/feature_9_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_9_matrix[1, 1]/feature_9_matrix[3, 1]\n",
    "        else:\n",
    "            class_0_score = class_0_score*feature_9_matrix[2, 0]/feature_9_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_9_matrix[2, 1]/feature_9_matrix[3, 1]\n",
    "            \n",
    "        # feature 10 probability\n",
    "        if feature_10_min <= data_point[10] and data_point[10] <= feature_10_min + (feature_10_max - feature_10_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_10_matrix[0, 0]/feature_10_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_10_matrix[0, 1]/feature_10_matrix[3, 1] \n",
    "        elif feature_10_min + (feature_10_max - feature_10_min)/3.0 <= data_point[10] and data_point[10] <= feature_10_min + 2.0*(feature_10_max - feature_10_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_10_matrix[1, 0]/feature_10_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_10_matrix[1, 1]/feature_10_matrix[3, 1]\n",
    "        else:\n",
    "            class_0_score = class_0_score*feature_10_matrix[2, 0]/feature_10_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_10_matrix[2, 1]/feature_10_matrix[3, 1]\n",
    "            \n",
    "        # feature 11 probability\n",
    "        if feature_11_min <= data_point[11] and data_point[11] <= feature_11_min + (feature_11_max - feature_11_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_11_matrix[0, 0]/feature_11_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_11_matrix[0, 1]/feature_11_matrix[3, 1]         \n",
    "        elif feature_11_min + (feature_11_max - feature_11_min)/3.0 <= data_point[11] and data_point[11] <= feature_11_min + 2.0*(feature_11_max - feature_11_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_11_matrix[1, 0]/feature_11_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_11_matrix[1, 1]/feature_11_matrix[3, 1]\n",
    "        else:\n",
    "            class_0_score = class_0_score*feature_11_matrix[2, 0]/feature_11_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_11_matrix[2, 1]/feature_11_matrix[3, 1]\n",
    "\n",
    "        # feature 12 probability\n",
    "        if feature_12_min <= data_point[12] and data_point[12] <= feature_12_min + (feature_12_max - feature_12_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_12_matrix[0, 0]/feature_12_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_12_matrix[0, 1]/feature_12_matrix[3, 1]\n",
    "        elif feature_12_min + (feature_12_max - feature_12_min)/3.0 <= data_point[12] and data_point[12] <= feature_12_min + 2.0*(feature_12_max - feature_12_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_12_matrix[1, 0]/feature_12_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_12_matrix[1, 1]/feature_12_matrix[3, 1]\n",
    "        else:\n",
    "            class_0_score = class_0_score*feature_12_matrix[2, 0]/feature_12_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_12_matrix[2, 1]/feature_12_matrix[3, 1]\n",
    "            \n",
    "        # feature 13 probability\n",
    "        if feature_13_min <= data_point[13] and data_point[13] <= feature_13_min + (feature_13_max - feature_13_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_13_matrix[0, 0]/feature_13_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_13_matrix[0, 1]/feature_13_matrix[3, 1]        \n",
    "        elif feature_13_min + (feature_13_max - feature_13_min)/3.0 <= data_point[13] and data_point[13] <= feature_13_min + 2.0*(feature_13_max - feature_13_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_13_matrix[1, 0]/feature_13_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_13_matrix[1, 1]/feature_13_matrix[3, 1]\n",
    "        else:\n",
    "            class_0_score = class_0_score*feature_13_matrix[2, 0]/feature_13_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_13_matrix[2, 1]/feature_13_matrix[3, 1]\n",
    "            \n",
    "        # feature 14 probability\n",
    "        if feature_14_min <= data_point[14] and data_point[14] <= feature_14_min + (feature_14_max - feature_14_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_14_matrix[0, 0]/feature_14_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_14_matrix[0, 1]/feature_14_matrix[3, 1]         \n",
    "        elif feature_14_min + (feature_14_max - feature_14_min)/3.0 <= data_point[14] and data_point[14] <= feature_14_min + 2.0*(feature_14_max - feature_14_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_14_matrix[1, 0]/feature_14_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_14_matrix[1, 1]/feature_14_matrix[3, 1]\n",
    "        else:\n",
    "            class_0_score = class_0_score*feature_14_matrix[2, 0]/feature_14_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_14_matrix[2, 1]/feature_14_matrix[3, 1]\n",
    "            \n",
    "        # feature 15 probability\n",
    "        if feature_15_min <= data_point[15] and data_point[15] <= feature_15_min + (feature_15_max - feature_15_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_15_matrix[0, 0]/feature_15_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_15_matrix[0, 1]/feature_15_matrix[3, 1]        \n",
    "        elif feature_15_min + (feature_15_max - feature_15_min)/3.0 <= data_point[15] and data_point[15] <= feature_15_min + 2.0*(feature_15_max - feature_15_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_15_matrix[1, 0]/feature_15_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_15_matrix[1, 1]/feature_15_matrix[3, 1]\n",
    "        else:\n",
    "            class_0_score = class_0_score*feature_15_matrix[2, 0]/feature_15_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_15_matrix[2, 1]/feature_15_matrix[3, 1]\n",
    "            \n",
    "        # feature 16 probability\n",
    "        if feature_16_min <= data_point[16] and data_point[16] <= feature_16_min + (feature_16_max - feature_16_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_16_matrix[0, 0]/feature_16_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_16_matrix[0, 1]/feature_16_matrix[3, 1]        \n",
    "        elif feature_16_min + (feature_16_max - feature_16_min)/3.0 <= data_point[16] and data_point[16] <= feature_16_min + 2.0*(feature_16_max - feature_16_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_16_matrix[1, 0]/feature_16_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_16_matrix[1, 1]/feature_16_matrix[3, 1]\n",
    "        else:\n",
    "            class_0_score = class_0_score*feature_16_matrix[2, 0]/feature_16_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_16_matrix[2, 1]/feature_16_matrix[3, 1]\n",
    "            \n",
    "        # feature 17 probability\n",
    "        if feature_17_min <= data_point[17] and data_point[17] <= feature_17_min + (feature_17_max - feature_17_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_17_matrix[0, 0]/feature_17_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_17_matrix[0, 1]/feature_17_matrix[3, 1] \n",
    "        elif feature_17_min + (feature_17_max - feature_17_min)/3.0 <= data_point[17] and data_point[17] <= feature_17_min + 2.0*(feature_17_max - feature_17_min)/3.0:\n",
    "            class_0_score = class_0_score*feature_17_matrix[1, 0]/feature_17_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_17_matrix[1, 1]/feature_17_matrix[3, 1]\n",
    "        else:\n",
    "            class_0_score = class_0_score*feature_17_matrix[2, 0]/feature_17_matrix[3, 0]\n",
    "            class_1_score = class_1_score*feature_17_matrix[2, 1]/feature_17_matrix[3, 1]\n",
    "            \n",
    "        # determine actual class\n",
    "        actual_class = int(data_point[19])\n",
    "        # pick max class\n",
    "        if class_0_score > class_1_score:\n",
    "            confusion_matrix[0, actual_class] = confusion_matrix[0, actual_class] + 1\n",
    "            \n",
    "            # increment num correct\n",
    "            if actual_class == 0:\n",
    "                num_correct = num_correct + 1\n",
    "        else:\n",
    "            confusion_matrix[1, actual_class] = confusion_matrix[1, actual_class] + 1\n",
    "\n",
    "            # increment num correct\n",
    "            if actual_class == 1:\n",
    "                num_correct = num_correct + 1\n",
    "            \n",
    "            \n",
    "    accuracy[i] = num_correct/(end - start)\n",
    "    print('accuracy fold ', i, ': ', accuracy[i])\n",
    "    start = end + 1\n",
    "    end = end + 230\n",
    "    \n",
    "print('average accuracy: ', accuracy.mean())    \n",
    "print('confusion matrix:\\n', confusion_matrix)\n",
    "print('precision class 1: ', confusion_matrix[1,1]/(confusion_matrix[1,1] + confusion_matrix[1, 0]))\n",
    "print('recall class 1: ', confusion_matrix[1,1]/(confusion_matrix[1,1] + confusion_matrix[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: K Nearest Neighbor (KNN) Classifier\n",
    "\n",
    "The KNN classifier consists of two stages:-\n",
    "- In the training stage, the classifier takes the training data and simply memorizes it\n",
    "- In the test stage, the classifier compares the test data with the training data and simply returns the maximum occuring label of the k nearest data points.\n",
    "\n",
    "The distance calculation method is central to the algorithm, typically Euclidean distance is used but other distance metrics like Manhattan distance can also be used. In this assignment you'll be implementing the classifier using the Euclidean distance metric. It is important to note that Euclidean distance is very sensitive to the scaling of different attributes hence, before you can build your classifier you have to normalize the values of each feature in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Normalize the dataset so that each feature value lies between `[0-1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn_data():\n",
    "    # get data\n",
    "    data = get_data('messidor_features.txt')\n",
    "\n",
    "    # normalize values\n",
    "    normalized_df = (data - data.min())/(data.max() - data.min())\n",
    "    \n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Build your KNN classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(data, test_data, k):\n",
    "    # reset df index\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    # calculate euclidean distances and sort them\n",
    "    distances = (((test_data[0] - data[0])**2 + (test_data[1] - data[1])**2 + (test_data[2] - data[2])**2 + (test_data[3] - data[3])**2 + (test_data[4] - data[4])**2 + (test_data[5] - data[5])**2 + (test_data[6] - data[6])**2 + (test_data[7] - data[7])**2 + (test_data[8] - data[8])**2 + (test_data[9] - data[9])**2 + (test_data[10] - data[10])**2 + (test_data[11] - data[11])**2 + (test_data[12] - data[12])**2 + (test_data[13] - data[13])**2 + (test_data[14] - data[14])**2 + (test_data[15] - data[15])**2 + (test_data[16] - data[16])**2 + (test_data[17] - data[17])**2)**(1/2))\n",
    "\n",
    "    # determine class of k nearest neighbors\n",
    "    num_class_0 = 0\n",
    "    num_class_1 = 0\n",
    "    \n",
    "    if k > 1:\n",
    "        classes_count = data.iloc[distances.nsmallest(k).index, 19].value_counts()\n",
    "        if classes_count.index[0] == 0.0:\n",
    "            num_class_0 = classes_count.values[0]\n",
    "        else:\n",
    "            num_class_1 = classes_count.values[0]\n",
    "            \n",
    "        if classes_count.shape[0] > 1:\n",
    "            if classes_count.index[1] == 0.0:\n",
    "                num_class_0 = num_class_0 + classes_count.values[1]\n",
    "            else:\n",
    "                num_class_1 = num_class_1 + classes_count.values[1]\n",
    "            \n",
    "        if num_class_0 > num_class_1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1        \n",
    "    else:\n",
    "        return int(data.iloc[distances.nsmallest(k).index, 19].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Find the best value of k for this data. Try k ranging from 1 to 10. For each k value, use a 5-fold cross validation to evaluate the accuracy with that k. In each fold of CV, divide your data into a training set and a validation set. Print out the best value of k and the accuracy achieved with that value. Return the best value of k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k(data):\n",
    "    # initialize accuracy array\n",
    "    accuracy = np.zeros((9, 1))\n",
    "\n",
    "    # try k values from 1 to 10 (odd numbers only)\n",
    "    kval = 1\n",
    "    while kval <= 9:\n",
    "        # intervals for training data and test data\n",
    "        start = 0\n",
    "        end = int(len(data.index)/5.0)\n",
    "\n",
    "        # initialize fold accuracy array\n",
    "        fold_accuracy = np.zeros((5, 1))\n",
    "\n",
    "        # 5 fold CV\n",
    "        for i in range(0, 5):\n",
    "            # initialize accuracy\n",
    "            num_correct = 0\n",
    "\n",
    "            # split data\n",
    "            test_data = data.iloc[start:end, :]\n",
    "            training_data = pd.concat([data.iloc[0:start, :], data.iloc[end:len(data.index), :]])\n",
    "\n",
    "            # test data\n",
    "            for index, data_point in test_data.iterrows():\n",
    "                class_num = knn(training_data, data_point, kval)\n",
    "                if class_num == int(data_point[19]):\n",
    "                    num_correct = num_correct + 1\n",
    "            \n",
    "            # calculate accuracy for fold\n",
    "            fold_accuracy[i] = num_correct/(end - start)\n",
    "            start = end + 1\n",
    "            end = end + 230\n",
    "        \n",
    "        # calculate average accuracy for k\n",
    "        accuracy[kval - 1] = fold_accuracy.mean()\n",
    "        \n",
    "        # increment k\n",
    "        kval = kval + 2\n",
    "\n",
    "        \n",
    "    # print best accuracy and best k\n",
    "    best_accuracy = np.max(accuracy)\n",
    "    print('\\tbest accuracy: ', best_accuracy)\n",
    "    best_k = np.where(accuracy == best_accuracy)\n",
    "    best_k = int(best_k[0]) + 1\n",
    "    print('\\tbest k: ', best_k)\n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Now measure the accuracy of your classifier using 5-fold cross validation. In each fold of this CV, divide your data into a training set and a test set. The training set should get sent through your code for Q4, resulting in a value of k to use. Using that k, calculate an accuracy on the test set. You will average the accuracy over all 5 folds to obtain the final accuracy measurement. Print the accuracy as well as the precision and recall for class label 1 (patients that have been diagnosed with the disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0 :\n",
      "\tbest accuracy:  0.5311515093981394\n",
      "\tbest k:  9\n",
      "Precision Class 1:  0.6782608695652174\n",
      "Recall Class 1:  0.609375\n",
      "Fold  1 :\n",
      "\tbest accuracy:  0.5557622935257263\n",
      "\tbest k:  5\n",
      "Precision Class 1:  0.6881720430107527\n",
      "Recall Class 1:  0.5203252032520326\n",
      "Fold  2 :\n",
      "\tbest accuracy:  0.5284744636415416\n",
      "\tbest k:  9\n",
      "Precision Class 1:  0.7053571428571429\n",
      "Recall Class 1:  0.6583333333333333\n",
      "Fold  3 :\n",
      "\tbest accuracy:  0.5396335674957281\n",
      "\tbest k:  3\n",
      "Precision Class 1:  0.6517857142857143\n",
      "Recall Class 1:  0.5934959349593496\n",
      "Fold  4 :\n",
      "\tbest accuracy:  0.5413613062464401\n",
      "\tbest k:  9\n",
      "Precision Class 1:  0.7053571428571429\n",
      "Recall Class 1:  0.6810344827586207\n",
      "\n",
      "\n",
      "Final Accuracy in Outer Loop:  0.6439984811087907\n"
     ]
    }
   ],
   "source": [
    "data = get_knn_data()\n",
    "\n",
    "# initialize accuracy array\n",
    "accuracy = np.zeros((5, 1))\n",
    "\n",
    "# initialize training and test intervals\n",
    "start = 0\n",
    "end = int(len(data.index)/5.0)\n",
    "\n",
    "# 5 fold CV\n",
    "for i in range(0, 5):\n",
    "    print('Fold ', i, ':')\n",
    "    \n",
    "    # initialize accuracy, precision, recall\n",
    "    num_correct = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    \n",
    "    # split data\n",
    "    test_data = data.iloc[start:end, :]\n",
    "    training_data = pd.concat([data.iloc[0:start, :], data.iloc[end:len(data.index), :]])\n",
    "    \n",
    "    # find best k on training data\n",
    "    best_k = find_best_k(training_data)\n",
    "    \n",
    "    # test using validation data\n",
    "    for index, data_point in test_data.iterrows():\n",
    "        class_num = knn(training_data, data_point, best_k)\n",
    "        if class_num == int(data_point[19]):\n",
    "            num_correct = num_correct + 1\n",
    "    \n",
    "        # increment count for precision and recall class 1\n",
    "        if class_num == 1 and int(data_point[19]) == 1:\n",
    "            true_positives = true_positives + 1\n",
    "        elif class_num == 1 and int(data_point[19]) != 1:\n",
    "            false_positives = false_positives + 1\n",
    "        elif class_num == 0 and int(data_point[19]) == 1:\n",
    "            false_negatives = false_negatives + 1\n",
    "            \n",
    "    # calculate accuracy of fold\n",
    "    accuracy[i] = num_correct/(end - start)\n",
    "    \n",
    "    # increment split intervals\n",
    "    start = end + 1\n",
    "    end = end + 230\n",
    "    \n",
    "    # calculate precision and recall for fold\n",
    "    precision_class_1 = true_positives/(true_positives + false_positives)\n",
    "    recall_class_1 = true_positives/(true_positives + false_negatives)\n",
    "    \n",
    "    # print precision and recall for class 1\n",
    "    print('Precision Class 1: ', precision_class_1)\n",
    "    print('Recall Class 1: ', recall_class_1)\n",
    "\n",
    "avg_accuracy = accuracy.mean()\n",
    "print('\\n\\nFinal Accuracy in Outer Loop: ', avg_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
